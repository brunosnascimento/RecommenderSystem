{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Based Recommendations Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook aims to show how to use sequence-based-recommendation module created by Robin Devooght. Some of these functions have been utilized in recent papers provided by the authors tackling sequence-based recommender systems \n",
    "\n",
    "<par>All code can be accessed in https://github.com/rdevooght/sequence-based-recommendations but a few modifications have been made</par>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "The available methods are:\n",
    "* [Recurrent Neural Networks](#recurrent-neural-networks)\n",
    "* [Stacked Denoising Autoencoder](#stacked-denoising-autoencoders)\n",
    "* [Latent Tarjectory Modeling/word2vec](#latent-trajectory-modeling)\n",
    "* [BPR-MF](#bpr-mf)\n",
    "* [FPMC](#fpmc)\n",
    "* [FISM](#fism)\n",
    "* [Fossil](#fossil)\n",
    "* [Markov Chains](#markov-chain)\n",
    "* [User KNN](#user-knn)\n",
    "* [Popularity baseline](#pop)\n",
    "\n",
    "### Neural Networks\n",
    "#### Recurrent Neural Networks\n",
    "\n",
    "Use it with `-m RNN`.\n",
    "The RNN have many options allowing to change the type/size/number of layers, the training procedure and the objective function, and some options are specific to a particular objective function.\n",
    "\n",
    "##### Layers\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`--r_t [LSTM, GRU, Vanilla]` | Type of recurrent layer (default is GRU)\n",
    "`--r_l size_of_layer1-size_of_layer2-etc.` | Size and number of layers. for example, `--r_l 100-50-50` creates a layer with 50 hidden neurons on top of another layer with 50 hidden neurons on top of a layer with 100 hidden neurons. Default: 32.\n",
    "`--r_bi` | Use bidirectional layers.\n",
    "`--r_emb size` | Adds an embedding layer before the recurrent layer. By default no embedding layer is used, but it is adviced to use one (e.g. `--r_emb 100`).\n",
    "\n",
    "##### Update mechanism\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`--u_m [adagrad, adadelta, rmsprop, nesterov, adam]` | Update mechanism (see [Lasagne doc](http://lasagne.readthedocs.io/en/latest/modules/updates.html)). Default is adam\n",
    "`--u_l float` | Learning rate (default: 0.001). The default learning rate works well with adam. For adagrad `--u_l 0.1` is adviced.\n",
    "`--u_rho float` | rho parameter for Adadelta and RMSProp, or momentum for Nesterov momentum (default: 0.9).\n",
    "`--u_b1 float` | Beta 1 parameter for Adam (default: 0.9).\n",
    "`--u_b2 float` | Beta 2 parameter for Adam (default: 0.999).\n",
    "\n",
    "##### Noise\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`--n_dropout P` | Dropout probability (default: 0.)\n",
    "`--n_shuf P` | Probability that an item is swapped with another one (default: 0.).\n",
    "`--n_shuf_std STD` | If an item is swapped, the position of the other item is drawn from a normal distribution whose std is defined by this parameter (default: 5.).\n",
    "\n",
    "##### Other options\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`-b int` | Size of the mini-batchs (default: 16)\n",
    "`--max_length int` | Maximum length of sequences (default: 200)\n",
    "`-g val` | Gradient clipping (default: 100)\n",
    "`--repeated_interactions` | Use when a user can interact multiple times with the same item. If not set, the items that the user already saw are never recommended.\n",
    "\n",
    "##### Objective functions\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`--loss [CCE, Blackout, TOP1, BPR, hinge, logit, logsig]` | Objective function. CCE is the categorical cross-entropy, BPR, TOP1 and Blackout are based on sampling, and hinge, logit and logsig allow to have multiple targets. Default is CCE.\n",
    "`-r float` | *Only for CCE*. Add a regularization term. A positive value will use L2 regularization and a negative value will use L1. Default: 0.\n",
    "`--db float` | *Only for CCE, Blackout, BPR and TOP1*. Increase the diversity bias to put more pressure on learning correct recomendations for unfrequent items (default: 0.).\n",
    "`--sampling float or int` | *Only for Blackout, BPR and TOP1*. Number of items to sample in the error computation. Use a float in [0,1] to express it as a fraction of the number of items in the catalog, or an int > 0 to specify the number of samples directly. Default: 32.\n",
    "`--n_targets N` | *Only for hinge, logit and logsig*. Number of items in the sequence that are used as targets. Default: 1.\n",
    "\n",
    "##### Clustering\n",
    "\n",
    "It is possible to combine RNNs with an item-clustering method. This leads to faster prediction on large dataset and creates meaningful item clusters.\n",
    "In order to use it, use the option `--clusters nb_of_clusters`.  \n",
    "For example, `python train.py -d path/to/dataset/ -m RNN --loss BPR --clusters 10` will train an RNN with the BPR loss and 10 clusters of items.\n",
    "Note that the clustering is only compatible with sampling-based loss (BPR, Blackout and TOP1). \n",
    "It also works with `--loss CCE`, but a sampling version of CCE is then used instead of the normal categorical cross-entropy.\n",
    "\t\n",
    "\n",
    "#### Stacked Denoising Autoencoders\n",
    "\n",
    "Use it with `-m SDAE`.\n",
    "SDAE the RNN options described in \"[Update mechanism](#update-mechanism)\" and \"[Other options](#other-options)\".\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`--L size_of_layer1-size_of_layer2-etc.` | Size and number of layers. for example, `--r_l 50-32-50` creates a layer with 50 hidden neurons on top of another layer with 32 hidden neurons on top of a layer with 50 hidden neurons. Default: 20.\n",
    "`--in_do float` | Dropout rate applied to the input layer of the SDAE (default: 0.2).\n",
    "`--do float` | Dropout rate applied to the hidden layers of the SDAE (default: 0.5).\n",
    "\n",
    "#### Latent Trajectory Modeling\n",
    "\n",
    "Use it with `-m  LTM`.\n",
    "LTM is a method based on word2vec, described in \"[Latent Trajectory Modeling: A Light and Efficient Way to Introduce Time in Recommender Systems](http://dl.acm.org/citation.cfm?id=2799676)\".\n",
    "LTM works in two steps: it first produces an embedding of the items with the word2vec algorithm using the sequence of items in the training set, then it estimates for each user a translation vector that would best explain the trajectory of that user in the embedded space.\n",
    "Predictions are made by finding the closest items to the last user item translated by the user's translation vector.\n",
    "Our implementation is mainly a wrapper around [Gensim's word2vec implementation](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`-H int` | Number of neurons (default: 20).\n",
    "`--ltm_window int` | Size of word2vec's window (default: 5).\n",
    "`--ltm_damping float` | Temporal damping (default: 0.8).\n",
    "`--ltm_no_trajectory` | Use this option to make predictions directly with word2vec, without the trajectory estimation proposed in the LTM paper.\n",
    "\n",
    "### Factorization-based\n",
    "#### FPMC\n",
    "\n",
    "FPMC is a method combining factorized markov chains with the factorization of the user-item matrix (see \"Factorizing personalized Markov chains for next-basket recommendation\" by Rendle et al. in *Proceedings of WWW'10*).\n",
    "Use it with `-m FPMC`\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`--k_cf int` | Rank of the user-item matrix factorization (default: 32).\n",
    "`--k_mc int` | Rank of the factorized Markov chain (default: 32).\n",
    "`-l val` | Learning rate (default: 0.01).\n",
    "`--cooling val` | Multiplicative factor applied to the learning rate after each epoch (default: 1)\n",
    "`--init_sigma val` | Standard deviation of the gaussian initialization (default: 1).\n",
    "`--fpmc_bias val` | Sampling bias (default: 100). By default the SGD process uses adaptive sampling to speed up learning. This parameter is used to control how much the sampling is biased towards high error items.\n",
    "`--no_adaptive_sampling` | No adaptive sampling\n",
    "`-r float` | Add a regularization term. A positive value will use L2 regularization and a negative value will use L1. Default: 0.\n",
    "\n",
    "#### BPR-MF\n",
    "\n",
    "BPR-MF is a matrix factorization method based on the BPR loss (see \"BPR: Bayesian personalized ranking from implicit feedback\" by Rendle et al. in *Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence*)\n",
    "Use it with `-m BPRMF`\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`-H int` | Rank of the user-item matrix factorization (default: 20).\n",
    "`-l val` | Learning rate (default: 0.01).\n",
    "`--cooling val` | Multiplicative factor applied to the learning rate after each epoch (default: 1)\n",
    "`--init_sigma val` | Standard deviation of the gaussian initialization (default: 1).\n",
    "`--fpmc_bias val` | Sampling bias (default: 100). By default the SGD process uses adaptive sampling to speed up learning. This parameter is used to control how much the sampling is biased towards high error items.\n",
    "`--no_adaptive_sampling` | No adaptive sampling\n",
    "`-r float` | Add a regularization term. A positive value will use L2 regularization and a negative value will use L1. Default: 0.\n",
    "\n",
    "#### FISM\n",
    "\n",
    "FISM is a method based of item-item factorization (see \"Fism: factored item similarity models for top-n recommender systems\" by Kabbur et al. in *Proceedings of SIGKDD'13*).\n",
    "It has the advantage over BPR-MF that it does not build a representation for each user. This leads to smaller models, and the ability to make recommendation to new users.\n",
    "Use it with `-m FISM --loss [BPR, RMSE]`\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`--loss [BPR, RMSE]` | Loss function. \"BPR\" is the same loss as for BPR-MF, \"RMSE\" optimizes the square error. This cannot be left to default because the default loss is CCE, which is not compatible with FISM.\n",
    "`-H int` | Rank of the matrix factorization (default: 20).\n",
    "`--fism_alpha float` | Alpha parameter in FISM. (default: 0.2).\n",
    "`-l val` | Learning rate (default: 0.01).\n",
    "`--cooling val` | Multiplicative factor applied to the learning rate after each epoch (default: 1)\n",
    "`--init_sigma val` | Standard deviation of the gaussian initialization (default: 1).\n",
    "`-r float` | Add a regularization term. A positive value will use L2 regularization and a negative value will use L1. Default: 0.\n",
    "\n",
    "FISM can be combined with item-clustering the same way that RNN can.\n",
    "To do so, add the option `--clusters nb_of_clusters`.\n",
    "When using clustering, a completely different implementation is used, which is based on Theano instead of Numpy.\n",
    "This has some implications on the available options:\n",
    "* The loss must be choosen among CCE, BPR, Blackout and TOP1 instead of BPR and RMSE.\n",
    "* The number of samples for each training step can be specified using `--sampling nb_of_samples`.\n",
    "* The update mechanism is controled by the options defined in [Update mechanism](#update-mechanism) instead of `-l` and `--cooling`.\n",
    "\n",
    "#### Fossil\n",
    "\n",
    "Fossil combines FISM with factorized markov chains (see \"Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation\" by He and McAuley in *Proceedings of ICDM'16*).\n",
    "Unlike FPMC, Fossil can use higher-order markov chains.\n",
    "Use it with `-m Fossil`\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`-H int` | Rank of the matrix factorization (default: 20).\n",
    "`--fism_alpha float` | Alpha parameter in FISM. (default: 0.2).\n",
    "`--fossil_order int` | Order of the markov chains in Fossil. (default: 1).\n",
    "`-l val` | Learning rate (default: 0.01).\n",
    "`--cooling val` | Multiplicative factor applied to the learning rate after each epoch (default: 1)\n",
    "`--init_sigma val` | Standard deviation of the gaussian initialization (default: 1).\n",
    "`-r float` | Add a regularization term. A positive value will use L2 regularization and a negative value will use L1. Default: 0.\n",
    "\n",
    "### Lazy\n",
    "\n",
    "**Lazy methods do not build models**, they make recommendation directly based on the dataset.\n",
    "They should therefor not be used with `train.py`, but only with `test.py`.\n",
    "\n",
    "#### POP\n",
    "\n",
    "Use it with `-m POP`.\n",
    "Always predict the most popular items.\n",
    "\n",
    "#### Markov Chain\n",
    "\n",
    "Use it with `-m MM`.\n",
    "Recommends the items that follow most often the last item the user's sequence.\n",
    "\n",
    "#### User KNN\n",
    "\n",
    "Use it with `-m UKNN`.\n",
    "User-based nearest neighbors approach. \n",
    "The similarity measure between users is the cosine similarity: #number-of-common-items / sqrt(#number-of-items-of-user-a * #number-of-items-of-user-b).\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`--ns int` | Neighborhood size (default: 80).\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import joblib\n",
    "import os.path\n",
    "import sys\n",
    "sources_path = './../Sources'\n",
    "sys.path.append('./../Sources/Devooght/')\n",
    "if sources_path not in sys.path:\n",
    "    sys.path.append(sources_path)\n",
    "\n",
    "import Devooght.preprocess as preprocess\n",
    "\n",
    "\n",
    "#from Evaluations.metrics import MSE, RMSE, MAE\n",
    "import os\n",
    "#sys.path.append('../Sources') # Adding my own libs source\n",
    "#from StackedAutoEncoderRS.SAERS import SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "results_path = './Results/'\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________\n",
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = '100k/' #100k dataset path\n",
    "#dataset = '10M/' #10M dataset path\n",
    "#dataset = '20M/' #20M dataset path\n",
    "\n",
    "dataset_path = '../../Datasets/movieLens/' + dataset; # Full dataset path\n",
    "\n",
    "filenames = {'movie': 'movie.csv', 'rating': 'rating.csv'}\n",
    "\n",
    "#df_ratings = preprocess.load_data(filename = dataset_path+filenames['rating'], \n",
    "#                       columns = ['userId', 'itemId', 'rating', 'timestamp'], \n",
    "#                       separator = ',',\n",
    "#                        verbose = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "This script (preprocess.py) takes a file containing a dataset of user/item interactions and split it into training/validation/test sets and save them in the format used by train.py and test.py.\n",
    "The original dataset must be in a format where each line correspond to a single user/item interaction.\n",
    "\n",
    "The only required argument is `-f path/to/dataset`, which is used to specify the original dataset. The script will create subfolders named \"data\", \"models\" and \"results\" in the folder containing the original dataset. \"data\" is used by preprocess.py to store all the files it produces, \"models\" is used by train.py to store the trained models and \"results\" is used by test.py to store the results of the tests.\n",
    "\n",
    "The optional arguments are the following:\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`--columns` | Order of the columns in the file (eg: \"uirt\"), u for user, i for item, t for timestamp, r for rating. If r is not present a default rating of 1 is given to all interaction. If t is not present interactions are assumed to be in chronological order. Extra columns are ignored. Default: uit\n",
    "`--sep` | Separator between the column. If unspecified pandas will try to guess the separator\n",
    "`--min_user_activity` | Users with less interactions than this will be removed from the dataset. Default: 2\n",
    "`--min_item_pop` | Items with less interactions than this will be removed from the dataset. Default: 5\n",
    "`--val_size` | Number of users to put in the validation set. If in (0,1) it will be interpreted as the fraction of total number of users. Default: 0.1\n",
    "`--test_size` | Number of users to put in the test set. If in (0,1) it will be interpreted as the fraction of total number of users. Default: 0.1\n",
    "`--seed` | Seed for the random train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Devooght.preprocess as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "directory_path = os.path.join('.', 'Sequence_based_recommendation_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating Auxiliary Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Creating directories in ./Sequence_based_recommendation_files\n",
      "[ ] Creating data directory\n",
      "[ ] Creating models directory\n",
      "[ ] Creating results directory\n",
      "[+] Directories created.\n",
      "[*] Loading data...\n",
      "\tSorting timestamp column\n",
      "\tSort data in chronological order\n",
      "[+] Data Loaded\n",
      "       userId  itemId  rating           timestamp\n",
      "214       259     255       4 1997-09-20 03:05:10\n",
      "83965     259     286       4 1997-09-20 03:05:27\n",
      "43027     259     298       4 1997-09-20 03:05:54\n",
      "21396     259     185       4 1997-09-20 03:06:21\n",
      "82655     259     173       4 1997-09-20 03:07:23\n",
      "[*] Removing inactive users and rare items...\n",
      "[+] Rare users and items removed.\n",
      "CPU times: user 77.4 ms, sys: 16.2 ms, total: 93.6 ms\n",
      "Wall time: 187 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reload(preprocess)\n",
    "\n",
    "# Creating Directory path\n",
    "directory_path = os.path.join('.', 'Sequence_based_recommendation_files')\n",
    "\n",
    "preprocess.create_dirs(dirname=directory_path, verbose = verbose)\n",
    "\n",
    "data = preprocess.load_data(\n",
    "                filename = dataset_path+filenames['rating'], \n",
    "                columns = ['userId', 'itemId', 'rating', 'timestamp'], \n",
    "                separator = ',',\n",
    "                sort_time = True,\n",
    "                verbose = verbose)    \n",
    "\n",
    "data = preprocess.remove_rare_elements(\n",
    "                data = data, \n",
    "                min_user_activity = 2, \n",
    "                min_item_popularity = 5,\n",
    "                verbose = verbose)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the mapping of original user and item ids to numerical consecutive ids in dirname.\n",
    "NB: some users and items might have been removed in previous steps and will therefore not appear in the mapping.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>itemId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>u_original</th>\n",
       "      <th>i_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>258</td>\n",
       "      <td>253</td>\n",
       "      <td>4</td>\n",
       "      <td>1997-09-20 03:05:10</td>\n",
       "      <td>259</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83965</th>\n",
       "      <td>258</td>\n",
       "      <td>284</td>\n",
       "      <td>4</td>\n",
       "      <td>1997-09-20 03:05:27</td>\n",
       "      <td>259</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43027</th>\n",
       "      <td>258</td>\n",
       "      <td>296</td>\n",
       "      <td>4</td>\n",
       "      <td>1997-09-20 03:05:54</td>\n",
       "      <td>259</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21396</th>\n",
       "      <td>258</td>\n",
       "      <td>183</td>\n",
       "      <td>4</td>\n",
       "      <td>1997-09-20 03:06:21</td>\n",
       "      <td>259</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82655</th>\n",
       "      <td>258</td>\n",
       "      <td>171</td>\n",
       "      <td>4</td>\n",
       "      <td>1997-09-20 03:07:23</td>\n",
       "      <td>259</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  itemId  rating           timestamp u_original i_original\n",
       "214       258     253       4 1997-09-20 03:05:10        259        255\n",
       "83965     258     284       4 1997-09-20 03:05:27        259        286\n",
       "43027     258     296       4 1997-09-20 03:05:54        259        298\n",
       "21396     258     183       4 1997-09-20 03:06:21        259        185\n",
       "82655     258     171       4 1997-09-20 03:07:23        259        173"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = preprocess.save_index_mapping(data = data, \n",
    "                                     dirname = directory_path + '/', \n",
    "                                     separator = ',')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the data set into training, validation and test sets.\n",
    "Each user is in one and only one set.\n",
    "nb_val_users is the number of users to put in the validation set.\n",
    "nb_test_users is the number of users to put in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into training, validation and test sets...\n",
      "Save training, validation and test sets in the triplets format...\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, test_set = preprocess.split_data(\n",
    "    data = data, \n",
    "    nb_val_users = 0.1, # val_size\n",
    "    nb_test_users = 0.1, # test_size\n",
    "    dirname = directory_path + '/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the train/validation/test sets in the sequence format and save them.\n",
    "Also create the extended training sequences, which countains the first half of the sequences of users in the validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save the training set in the sequences format...\n",
      "Save the validation set in the sequences format...\n",
      "Save the test set in the sequences format...\n",
      "Save the extended training set in the sequences format...\n"
     ]
    }
   ],
   "source": [
    "preprocess.make_sequence_format(\n",
    "    train_set = train_set, \n",
    "    val_set = val_set, \n",
    "    test_set = test_set, \n",
    "    dirname = directory_path + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save stats...\n"
     ]
    }
   ],
   "source": [
    "preprocess.save_data_stats(\n",
    "                data = data, \n",
    "                train_set = train_set, \n",
    "                val_set = val_set, \n",
    "                test_set = test_set, \n",
    "                dirname = directory_path + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Readme file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.make_readme(dirname = directory_path + '/', \n",
    "                       val_set = val_set, \n",
    "                       test_set = test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following files were automatically generated by preprocess.py\n",
    "\n",
    "    user_id_mapping\n",
    "\t\tmapping between the users ids in the original dataset and the new users ids.\n",
    "\t\tthe first column contains the new id and the second the original id.\n",
    "        Inactive users might have been deleted from the original, and they will therefore not appear in the id mapping.\n",
    "\n",
    "\titem_id_mapping\n",
    "\t\tIdem for item ids.\n",
    "\n",
    "\ttrain_set_triplets\n",
    "\t\tTraining set in the triplets format.\n",
    "\t\tEach line is a user item interaction in the form (user_id, item_id, rating). \n",
    "\t\tInteractions are listed in chronological order.\n",
    "\n",
    "\ttrain_set_sequences\n",
    "\t\tTraining set in the sequence format.\n",
    "\t\tEach line contains all the interactions of a user in the form (user_id, first_item_id, first_rating, 2nd_item_id, 2nd_rating, ...).\n",
    "\n",
    "\ttrain_set_sequences+\n",
    "\t\tExtended training set in the sequence format.\n",
    "\t\tThe extended training set contains all the training set plus the first half of the interactions of each users in the validation and testing set.\n",
    "\n",
    "\tval_set_triplets\n",
    "\t\tValidation set in the triplets format\n",
    "\n",
    "\tval_set_triplets\n",
    "\t\tValidation set in the sequence format\n",
    "\n",
    "\ttest_set_triplets\n",
    "\t\tTest set in the triplets format\n",
    "\n",
    "\ttest_set_triplets\n",
    "\t\tTest set in the sequence format\n",
    "\n",
    "\tstats\n",
    "\t\tContains some informations about the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "This script is used to train models and offers many options regarding when to save new models and when to stop training.\n",
    "\n",
    "If you have multiple datasets with a partly common path (e.g. path/to/dataset1/, path/to/dataset2/, etc.) you can specify this common path in the variable DEFAULT_DIR of helpers/data_handling.py. For example, setting DEFAULT_DIR = \"path/to/\" and using the argument `-d dataset1` will look for the dataset in \"path/to/dataset1/\".\n",
    "\n",
    "The optional arguments are the following:\n",
    "\n",
    "Option | Desciption\n",
    "------ | ----------\n",
    "`--dir dirname/` | Name of the subfolder of \"path/to/dataset/models/\" in which to save the model. By default it will be saved directly in the models/ folder, but using subfolders can be useful when many models are tested.\n",
    "`--progress {int or float}` | Number of iterations (or seconds) between two evaluations of the model on the validation set. When the model is evaluated, progress is shown on the command line, and the model might be saved (depending on the `--save` option). An float value means that the evaluations happen at geometric intervals (rather than linear). Default: 2.0\n",
    "`--metrics value` | Metrics computed on the validation set, separated by commas. Available metrics are recall, sps, ndcg, item\\_coverage, user\\_coverage and blockbuster\\_share. Default: sps.\n",
    "`--save [All, Best, None]` | Policy for saving models. If \"None\", no model is saved. If \"All\", the current model is saved each time the model is evaluated on the validation set, and no model is destroyed. If \"Best\", the current model is only saved if it improves over the previous best results on the validation set, and the previous best model is deleted. If \"Best\" and multiple metrics are used, all the pareto-optimal models are saved. \n",
    "`--time_based_progress` | Base the interval between two evaluations on the number of elapsed seconds rather than on the number of iterations.\n",
    "`--mpi value` | Max number of iterations (or seconds) between two evaluations (useful when using geometric intervals). Default: inf.\n",
    "`--max_iter value` | Max number of iterations (default: inf).\n",
    "`--max_time value` | Max training time in seconds (default: inf).\n",
    "`--min_iter value` | Min number of iterations before making the first evaluation (default: 0).\n",
    "`--extended_set` | Use extended training set (contains first half of validation and test set). This is necessary for factorization based methods such as BPRMF and FPMC because they need to build a model for every user.\n",
    "`--tshuffle` | Shuffle the order of sequences between epochs.\n",
    "`--load_last_model` | Load Last model before starting training (it will search for a model build with all the same options and take the one with the largest number of epochs).\n",
    "`--es_m [WorstTimesX, StopAfterN, None]` | Early stopping method (by default none is used, and training continues until max_iter or max_time is reached). WorstTimesX will stop training if the number of iterations since the last best score on the validation set is longer than X times the longest time between two consecutive best scores. StopAfterN will stop the training if the model has not improved for the N last evaluations on the validation set.\n",
    "`--es_n N` | N parameter for StopAfterN (default: 5).\n",
    "`--es_x X` | X parameter for WorstTimesX (default: 2).\n",
    "`--es_min_wait num_epochs` | Mininum number of epochs before stopping (for WorstTimesX). Default: 1.\n",
    "`--es_LiB` | Lower is better for validation score. By default a higher validation score is considered better, but if it is not the case you can use this option.\n",
    "\n",
    "The options specific to each method are explained in the Methods section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Devooght.train' from './../Sources/Devooght/train.pyc'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Devooght.train as train\n",
    "reload(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "command_parser: a parser containing sub command parsers such as parser to predictor, training and early stopping\n",
    "\n",
    "sub_command_parser should be callables that will add arguments to the command parser\n",
    "\n",
    "4 sub commands parser are used:\n",
    "- **predictor_command_parser** from the helpers.command_parser module\n",
    "- **training_command_parser** from the train module\n",
    "- **early_stopping_command_parser** from the helpers.command_parser module\n",
    "- **test_command_parser** from the test module\n",
    "\n",
    "All commands are orchestraded by **command_parser** from the helpers.command_parser module\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers.command_parser as parse\n",
    "reload(parse)\n",
    "parser = parse.command_parser(parse.predictor_command_parser, \n",
    "                            train.training_command_parser, \n",
    "                            parse.early_stopping_command_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--dir', directory_path + '/models/', \n",
    "                          '-d', directory_path + '/', \n",
    "                          '-b', '16', # Batch size\n",
    "                          '--max_iter', '500', # Maximum number of iterations\n",
    "                          '--progress', '5', # when progress information should be printed during training\n",
    "                          '-m', 'FISM', # Method\n",
    "                          #'-i', '-1', # Number of batches - only on test parser\n",
    "                         '--loss', 'RMSE']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<factorization.fism.FISM at 0x7fbefaeaf090>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = parse.get_predictor(args)\n",
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.data_handling import DataHandler\n",
    "dataset = DataHandler(dirname = args.dataset, \n",
    "                      extended_training_set = args.extended_set, \n",
    "                      shuffle_training = args.tshuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.prepare_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Sequence_based_recommendation_files/models/'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.43201242271\n",
      "11.421367277\n",
      "-3.49185790744\n",
      "41.7549920293\n",
      "-3.49070297264\n",
      "Opening file (1)\n",
      "Algorithm: FISM \n",
      "Batch Number: 5 \n",
      "Epochs: 6.1820744569e-05 in 0.161010980606 s\n",
      "Last train cost : 10.1251621698\n",
      "recall :  0.00893640783269\n",
      "sps :  0.0333333333333\n",
      "Best  sps :  0.0333333333333\n",
      "blockbuster_share :  0.0\n",
      "item_coverage :  32\n",
      "ndcg :  0.0510741334933\n",
      "user_coverage :  0.333333333333\n",
      "-----------------\n",
      "Save model in ./Sequence_based_recommendation_files/models/fism_RMSE_ne0.0_lr0.01_an1.0_k20_reg0.0_ini1.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5 6.1820744569e-05 0.167484998703 10.1251621698 0.00893640783269 0.0333333333333 0.0 32 0.0510741334933 0.333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9947290406\n",
      "17.0375837018\n",
      "9.81669789443\n",
      "-3.78251015917\n",
      "2.05502850679\n",
      "-1.81939384625\n",
      "-16.9734189345\n",
      "26.1179689886\n",
      "-0.72237144696\n",
      "65.3489558935\n",
      "5.39581938182\n",
      "-1.96596463303\n",
      "24.8592633534\n",
      "-3.86274971111\n",
      "55.3304426455\n",
      "1.05261694111\n",
      "-3.36439082403\n",
      "-3.05659237551\n",
      "26.1578290094\n",
      "-7.33210883301\n",
      "Opening file (1)\n",
      "Algorithm: FISM \n",
      "Batch Number: 25 \n",
      "Epochs: 0.000309103722845 in 0.617953062057 s\n",
      "Last train cost : 10.3143717297\n",
      "recall :  0.00913315460071\n",
      "sps :  0.0111111111111\n",
      "Best  sps :  0.0333333333333\n",
      "blockbuster_share :  0.0909090909091\n",
      "item_coverage :  25\n",
      "ndcg :  0.0488774250298\n",
      "user_coverage :  0.322222222222\n",
      "-----------------\n",
      "31.8948481896\n",
      "-29.0658598474\n",
      "-25.1907178289\n",
      "5.02816544613\n",
      "22.8650594556\n",
      "-12.5415400357\n",
      "-12.0360136099\n",
      "21.7628502418\n",
      "3.14348073144\n",
      "-9.27025906414\n",
      "1.19105055959\n",
      "14.3701056142\n",
      "6.78144760679\n",
      "15.787492319\n",
      "-1.76758052614\n",
      "-8.68473630334\n",
      "-34.9861066429\n",
      "5.75760243219\n",
      "14.5579465731\n",
      "18.4592762729\n",
      "-14.616543604\n",
      "-1.07105229405\n",
      "8.28826531204\n",
      "-31.0866775513\n",
      "50.5960730579\n",
      "-2.75836966672\n",
      "23.9921163995\n",
      "-74.8276579506\n",
      "71.7725887152\n",
      "7.00718848606\n",
      "9.99081109337\n",
      "-4.84233752521\n",
      "-38.1690732851\n",
      "26.9380853385\n",
      "23.0033696241\n",
      "-48.1536431786\n",
      "-21.3783919552\n",
      "17.9625619274\n",
      "1.45919727669\n",
      "-19.1678450727\n",
      "-4.6946407461\n",
      "15.4649932042\n",
      "-3.36697563224\n",
      "14.0454161478\n",
      "-16.8195682151\n",
      "17.1271830871\n",
      "100.133088443\n",
      "-9.51895098213\n",
      "33.0264209212\n",
      "16.9656087916\n",
      "0.0304214808812\n",
      "26.3509343774\n",
      "-27.700558419\n",
      "-13.5890726504\n",
      "7.54144751425\n",
      "3.8240331014\n",
      "3.0213428241\n",
      "-2.25204555009\n",
      "26.7989929384\n",
      "12.196174325\n",
      "38.646362074\n",
      "5.95990404597\n",
      "5.31560882029\n",
      "-7.51140414526\n",
      "1.07923478437\n",
      "17.3571181172\n",
      "-33.8056088036\n",
      "-4.51656949716\n",
      "4.24452994334\n",
      "-50.3179834479\n",
      "-22.4401027901\n",
      "71.8637567999\n",
      "8.09143363531\n",
      "6.83309565768\n",
      "1.83674784129\n",
      "-18.5040292834\n",
      "-19.7060661645\n",
      "-11.1210759793\n",
      "-28.0674206747\n",
      "-65.3503402517\n",
      "12.7128626095\n",
      "59.4333791903\n",
      "2.25121414295\n",
      "13.180948316\n",
      "8.16838437194\n",
      "-58.286393452\n",
      "-3.31047183476\n",
      "18.1764170102\n",
      "4.36687477106\n",
      "-13.545930313\n",
      "63.9471465738\n",
      "144.221528175\n",
      "-162.664642404\n",
      "-11.7619370293\n",
      "21.393291279\n",
      "27.4880744081\n",
      "-7.63511358607\n",
      "15.6554391992\n",
      "39.0136522956\n",
      "29.0303682994\n",
      "Opening file (1)\n",
      "Algorithm: FISM \n",
      "Batch Number: 125 \n",
      "Epochs: 0.00154551861423 in 0.782687902451 s\n",
      "Last train cost : 3.13301704397\n",
      "recall :  0.00508537964925\n",
      "sps :  0.0\n",
      "Best  sps :  0.0333333333333\n",
      "blockbuster_share :  0.0\n",
      "item_coverage :  12\n",
      "ndcg :  0.0321625492571\n",
      "user_coverage :  0.255555555556\n",
      "-----------------\n",
      "-29.5226937291\n",
      "-71.6836811318\n",
      "-121.414517114\n",
      "-10.1890397529\n",
      "9.6733979077\n",
      "50.3419218193\n",
      "-96.5525609862\n",
      "101.266914376\n",
      "99.5480626505\n",
      "-170.226726572\n",
      "27.6259896814\n",
      "166.638261061\n",
      "-8.92812763975\n",
      "89.2952506062\n",
      "18.0458948495\n",
      "0.700109000717\n",
      "10.9976330876\n",
      "-171.781692244\n",
      "-105.988425703\n",
      "-72.9348479559\n",
      "23.941499558\n",
      "336.901739392\n",
      "-11.7041339762\n",
      "254.597295711\n",
      "34.5059950164\n",
      "305.214900549\n",
      "3.54928085096\n",
      "-213.21930083\n",
      "32.9100632338\n",
      "498.109625939\n",
      "90.065173924\n",
      "533.319307481\n",
      "-232.460134554\n",
      "61.0742661835\n",
      "-252.894667571\n",
      "-417.359588006\n",
      "479.437818172\n",
      "-1678.29525309\n",
      "-874.904765934\n",
      "35.5694958649\n",
      "-35.6524289477\n",
      "1956.38364815\n",
      "84.1504037561\n",
      "-376.74891918\n",
      "32.6832494015\n",
      "2134.24642008\n",
      "-115.046704532\n",
      "-604.896642479\n",
      "-28714.3772876\n",
      "78177.8514598\n",
      "-95970.8451918\n",
      "28902.3099077\n",
      "88284.5849853\n",
      "45075.8329081\n",
      "25492.780974\n",
      "-81021.422541\n",
      "-90133.4195688\n",
      "280089.292548\n",
      "49331.3668268\n",
      "-183994.222018\n",
      "-11939.2598676\n",
      "50419.6862511\n",
      "83940.8866949\n",
      "-298634.791322\n",
      "-59729.5145077\n",
      "-128913.249899\n",
      "14516.8541669\n",
      "-21314.724804\n",
      "-71645.9803769\n",
      "-373730.700732\n",
      "-21662.575261\n",
      "-184682.624991\n",
      "-191672.465481\n",
      "-15240.5501029\n",
      "124074.131048\n",
      "-29484.9645912\n",
      "-46872.8845316\n",
      "7727.6055704\n",
      "-176515.365023\n",
      "94755.5525626\n",
      "104117.13135\n",
      "-125588.8311\n",
      "17944.9141206\n",
      "264499.288932\n",
      "37731.6817465\n",
      "11739.1931338"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25 0.000309103722845 0.626373052597 10.3143717297 0.00913315460071 0.0111111111111 0.0909090909091 25 0.0488774250298 0.322222222222\n",
      "125 0.00154551861423 0.785217046738 3.13301704397 0.00508537964925 0.0 0.0 12 0.0321625492571 0.255555555556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-132021878746.0\n",
      "-6.16689310356e+15\n",
      "-2.05766290796e+16\n",
      "2.76997289798e+14\n",
      "-8.32908345959e+16\n",
      "3.95139607671e+16\n",
      "-1.10211089618e+16\n",
      "-1.74597807452e+15\n",
      "4.18164992329e+16\n",
      "6.41725830149e+16\n",
      "6.80815814549e+16\n",
      "-8.58493053876e+15\n",
      "-1.71017148697e+16\n",
      "1.22038063641e+16\n",
      "-2.52440111696e+16\n",
      "1.61855712867e+16\n",
      "-4.97987698209e+15\n",
      "4.95493008696e+15\n",
      "-4.18359569427e+16\n",
      "2.62124422543e+16\n",
      "1.72163026549e+16\n",
      "3.480929875e+16\n",
      "nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-1d212f16f470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_early_stopper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mload_last_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_last_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         validation_metrics=args.metrics.split(','))\n\u001b[0m",
      "\u001b[0;32m/home/guedes.joaofelipe/Workplace/ProjetoFinal/Codigos/Sources/Devooght/factorization/mf_base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, max_time, progress, time_based_progress, autosave, save_dir, min_iterations, max_iter, max_progress_interval, load_last_model, early_stopping, validation_metrics)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_time\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Aqui\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/guedes.joaofelipe/Workplace/ProjetoFinal/Codigos/Sources/Devooght/factorization/fism.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, iterations)\u001b[0m\n\u001b[1;32m    148\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc_sgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_auc_training_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse_sgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rmse_training_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/guedes.joaofelipe/Workplace/ProjetoFinal/Codigos/Sources/Devooght/factorization/fism.py\u001b[0m in \u001b[0;36mrmse_sgd_step\u001b[0;34m(self, user_items, item, rating)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NaN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;31m# print(prediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: NaN"
     ]
    }
   ],
   "source": [
    "metrics = predictor.train(dataset, \n",
    "        save_dir=args.dir, \n",
    "        time_based_progress=args.time_based_progress, \n",
    "        progress=float(args.progress), \n",
    "        autosave=args.save, \n",
    "        max_progress_interval=args.mpi, \n",
    "        max_iter = args.max_iter,\n",
    "        min_iterations=args.min_iter,\n",
    "        max_time=args.max_time,\n",
    "        early_stopping=parse.get_early_stopper(args),\n",
    "        load_last_model=args.load_last_model,\n",
    "        validation_metrics=args.metrics.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'recall': 0.010243067760207145, 'sps': 0.022222222222222223, 'blockbuster_share': 0.07317073170731707, 'item_coverage': 35, 'ndcg': 0.046645094099566568, 'user_coverage': 0.34444444444444444}\n",
      "Time: 0.638215065002\n",
      "Best filename: ./Sequence_based_recommendation_files/models/fism_RMSE_ne0.0_lr0.01_an1.0_k20_reg0.0_ini1.npz\n"
     ]
    }
   ],
   "source": [
    "print (\"Metrics: {}\".format(metrics[0]))\n",
    "print (\"Time: {}\".format(metrics[1]))\n",
    "print (\"Best filename: {}\".format(metrics[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "This script test the models built with train.py on the test set.\n",
    "The basic usage is:\n",
    "````\n",
    "python test.py -d path/to/dataset/ -m Method_name\n",
    "````\n",
    "The argument `-d` works in the same way as with train.py, and the precise model to test is specified by the `--dir` option and the methods-specific options.\n",
    "If multiple models fit the options (They are in the same subfolder and were trained with the same method and same options), they are all evaluated one after the other, except if the argument `-i epoch_number` is also specified, which will then select the model based on the number of epochs.\n",
    "\n",
    "`--metrics` allows to specify the list of metrics to compute, separated by commas. By default the metrics are: sps, recall, item\\_coverage, user\\_coverage, blockbuster_share.\n",
    "The \"blockbuster share\" is the percentage of correct recommendations among the 1% most popular items.\n",
    "The other available metrics are the sps, the ndcg and the assr (when clustering is used).\n",
    "\n",
    "All the metrics are computed \"@k\", with k=10 by default. k can be changed using the `-k` option.\n",
    "\n",
    "When the `--save` option is used, the results are saved in a file in \"path/to/dataset/results/\".\n",
    "the results of each model form a line of the file, and each line contains the number of epochs followed by the metrics specified by `--metrics`.\n",
    "\n",
    "When testing a method based on clustering, the option `--ignore_clusters` can be used to test how the method performs without clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Devooght.test' from './../Sources/Devooght/test.pyc'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Devooght.test as test\n",
    "reload(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Sequence_based_recommendation_files/models/./Sequence_based_recommendation_files/models/fism_RMSE_ne*_lr0.01_an1.0_k20_reg0.0_ini1.npz\n",
      "Opening file (1)\n",
      "Timer:  1.19\n",
      "-------------------\n",
      "(2/3) results on ./Sequence_based_recommendation_files/models/./Sequence_based_recommendation_files/models/fism_RMSE_ne0.001_lr0.01_an1.0_k20_reg0.0_ini1.npz\n",
      "sps@10:  0.0\n",
      "recall@10:  0.00475384593487\n",
      "item_coverage@10:  11\n",
      "user_coverage@10:  0.215909090909\n",
      "blockbuster_share@10:  0.0\n",
      "Opening file (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-\t0.0\t0.00475384593487\t11\t0.215909090909\t0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer:  1.26\n",
      "-------------------\n",
      "(3/3) results on ./Sequence_based_recommendation_files/models/./Sequence_based_recommendation_files/models/fism_RMSE_ne0.002_lr0.01_an1.0_k20_reg0.0_ini1.npz\n",
      "sps@10:  0.0227272727273\n",
      "recall@10:  0.0162544340951\n",
      "item_coverage@10:  15\n",
      "user_coverage@10:  0.477272727273\n",
      "blockbuster_share@10:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-\t0.0227272727273\t0.0162544340951\t15\t0.477272727273\t0.0\n"
     ]
    }
   ],
   "source": [
    "parser = parse.command_parser(parse.predictor_command_parser, \n",
    "                              test.test_command_parser)\n",
    "\n",
    "args = parser.parse_args(['--dir', directory_path + '/models/', \n",
    "                          '-d', directory_path + '/', \n",
    "                          '-b', '16', # Batch size                                                    \n",
    "                          '-m', 'FISM', # Method\n",
    "                          #'-i', '-1', # Number of batches - only on test parser\n",
    "                         '--loss', 'RMSE']) \n",
    "\n",
    "args.training_max_length = args.max_length\n",
    "# args.max_length = int(DATA_HANDLER.max_length/2)\n",
    "\n",
    "if args.number_of_batches == -1:\n",
    "    args.number_of_batches = \"*\"\n",
    "\n",
    "dataset = DataHandler(dirname=args.dataset)\n",
    "\n",
    "predictor = parse.get_predictor(args)\n",
    "\n",
    "predictor.prepare_model(dataset)\n",
    "\n",
    "file = test.find_models(predictor, dataset, args)\n",
    "\n",
    "if args.number_of_batches == \"*\" and args.method != \"UKNN\" and args.method != \"MM\" and args.method != \"POP\":\n",
    "\n",
    "    output_file = test.save_file_name(predictor, dataset, args)\n",
    "\n",
    "    last_tested_batch = test.get_last_tested_batch(output_file)\n",
    "    batches = np.array(map(test.extract_number_of_epochs, file))\n",
    "    sorted_ids = np.argsort(batches)\n",
    "    batches = batches[sorted_ids]\n",
    "    file = file[sorted_ids]\n",
    "    for i, f in enumerate(file):\n",
    "        if batches[i] > last_tested_batch:\n",
    "            evaluator = test.run_tests(predictor, f, dataset, args, get_full_recommendation_list=args.save_rank, k=args.nb_of_predictions)\n",
    "            print('-------------------')\n",
    "            print('({}/{}) results on {}'.format(i+1, len(file), f))            \n",
    "            test.print_results(evaluator, args.metrics.split(','), plot=False, file=output_file, n_batches=batches[i], print_full_rank_comparison=args.save_rank)\n",
    "else:\n",
    "    evaluator = run_tests(predictor, file, dataset, args, get_full_recommendation_list=args.save_rank, k=args.nb_of_predictions)\n",
    "    print_results(evaluator, args.metrics.split(','), file=save_file_name(predictor, dataset, args), print_full_rank_comparison=args.save_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# Creating a parser\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "\n",
    "# Adding arguments\n",
    "parser.add_argument('integers', metavar='N', type=int, nargs='+', help='an integer for the accumulator')\n",
    "parser.add_argument('--sum', dest='accumulate', action='store_const',\n",
    "                     const=sum, default=max,\n",
    "                     help='sum the integers (default: find the max)')\n",
    "parser.add_argument('--max', dest='accumulate', action='store_const',\n",
    "                     const=max, default=max,\n",
    "                     help='max the integers (default: find the max)')\n",
    "\n",
    "# Parsing arguments\n",
    "#parser.parse_args()\n",
    "parser.parse_args(['--sum', '7', '42'])\n",
    "parser.parse_args(['--max', '5', '4'])\n",
    "parser.epilog\n",
    "#parser.argument_default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
